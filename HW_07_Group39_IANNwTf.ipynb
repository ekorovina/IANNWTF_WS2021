{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW_07_Group39_IANNwTf.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8Fp8wE04IXiG"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def integration_task(seq_len, num_samples):\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "\n",
        "        x = tf.random.normal((seq_len, 1))\n",
        "        target = tf.expand_dims(tf.cast(tf.math.reduce_sum(x) > 0, tf.int16), -1)\n",
        "\n",
        "        yield x, target"
      ],
      "metadata": {
        "id": "-qP6N2nJIzqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = 10\n",
        "num_samples = 80000\n",
        "\n",
        "def my_integration_wrapper():\n",
        "\n",
        "  for x,y in integration_task(seq_len, num_samples):\n",
        "      yield x,y \n",
        "\n"
      ],
      "metadata": {
        "id": "9L_emDOC5T6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = tf.data.Dataset.from_generator(my_integration_wrapper, output_signature=(\n",
        "                                            tf.TensorSpec(shape=(seq_len,1), dtype= tf.float32), \n",
        "                                            tf.TensorSpec(shape=(1,), dtype=tf.int16)))\n"
      ],
      "metadata": {
        "id": "toveapJLRDfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare(ds):   \n",
        "    # Prepare data for model\n",
        "\n",
        "    # cache\n",
        "    ds = ds.cache()\n",
        "    # shuffle, batch, prefetch our dataset\n",
        "    ds = ds.shuffle(5000)\n",
        "    ds = ds.batch(256)\n",
        "    ds = ds.prefetch(128)\n",
        "    return ds"
      ],
      "metadata": {
        "id": "n6LT3KELRuQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare data\n",
        "ds = prepare(data)\n",
        "\n",
        "train_ds = ds.take(72000)\n",
        "test_ds = ds.skip(72000).take(8000)"
      ],
      "metadata": {
        "id": "VUsR4VndXi2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, BatchNormalization, Activation, GlobalAvgPool2D"
      ],
      "metadata": {
        "id": "lJQa8TbJYAQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#class LSTM_Cell(Layer):\n",
        "#\n",
        "#    def __init__(self, units):\n",
        "#        super(LSTM_Cell, self).__init__()\n",
        "#\n",
        "#        self.units = units\n",
        "#\n",
        "#        # forget gate\n",
        "#        self.f_gate = Dense(units, activation=\"sigmoid\", bias_initializer='ones')\n",
        "#\n",
        "#        # input gate \n",
        "#        self.i_gate = Dense(units, activation=\"tanh\")\n",
        "#\n",
        "#        # candidate gate\n",
        "#        self.c_gate = Dense(units, activation=\"sigmoid\")\n",
        "#\n",
        "#        # output gate\n",
        "#        self.o_gate = Dense(units, activation=\"sigmoid\")\n",
        "#\n",
        "#        def call(self, x, states):\n",
        "#\n",
        "#            # current cell state and hidden cell state\n",
        "#            hidden_state, cell_state = states\n",
        "#\n",
        "#            # put input and hidden state in a tensor \n",
        "#            input = tf.concat((x, hidden_state), axis=-1)\n",
        "#\n",
        "#            # compute forget gate output\n",
        "#            forget = self.f_gate(input)\n",
        "#\n",
        "#            # update the currentcell state\n",
        "#            cell_state = cell_state * forget\n",
        "#\n",
        "#            # compute input_gate output * candidate gate output \n",
        "#            candidate = self.i_gate(input) * self.c_gate(input)\n",
        "#\n",
        "#            # update cell state\n",
        "#            cell_state = cell_state + candidate \n",
        "#\n",
        "#            # compute output\n",
        "#            output = tf.tanh(cell_state) * self.o_gate(input)\n",
        "#\n",
        "#            return output, (output, cell_state)\n",
        "\n",
        "class LSTM_Cell(Layer):\n",
        "  def __init__(self, units):\n",
        "    super(LSTM_Cell, self).__init__()\n",
        "    self.units = units\n",
        "    self.forget_Gate = tf.keras.layers.Dense(units, activation=tf.nn.sigmoid, bias_initializer=tf.keras.initializers.Ones)\n",
        "    self.candidates = tf.keras.layers.Dense(units, activation=tf.nn.tanh, kernel_initializer='orthogonal')\n",
        "    self.candidates_gate = tf.keras.layers.Dense(units, activation=tf.nn.sigmoid, kernel_initializer='orthogonal')\n",
        "    self.out_gate = tf.keras.layers.Dense(units, activation=tf.nn.sigmoid, kernel_initializer='orthogonal')\n",
        "  \n",
        "  @tf.function\n",
        "  def call(self, x, states):\n",
        "    hidden_state, cell_state = states\n",
        "    concat_input = tf.concat((x, hidden_state), axis=-1)\n",
        "    cell_state = cell_state*self.forget_Gate(concat_input)\n",
        "\n",
        "\n",
        "    update = self.candidates(concat_input)*self.candidates_gate(concat_input)\n",
        "    cell_state = cell_state + update\n",
        "    out = tf.nn.tanh(cell_state)*self.out_gate(concat_input)\n",
        "    return out, (out, cell_state)"
      ],
      "metadata": {
        "id": "pjoH-uM0VyVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM_Layer(Layer):\n",
        "    def __init__(self, cell):\n",
        "        super(LSTM_Layer, self).__init__()\n",
        "        self.cell = cell\n",
        "    @tf.function\n",
        "    def call(self, x, states):\n",
        "        \n",
        "        seq_len = tf.shape(x)[1]\n",
        "        #print(seq_len)\n",
        "\n",
        "        output = tf.TensorArray(dtype=tf.float32, size=seq_len, clear_after_read=True)\n",
        "\n",
        "        for i in tf.range(seq_len):\n",
        "            #print(self.cell(x[:, i, :], states))\n",
        "            out, states = self.cell(x[:,i,:], states)\n",
        "            output = output.write(i, out)\n",
        "        result = output.stack()\n",
        "        #print(result)\n",
        "        # perm=[1,0,2]?\n",
        "        result = tf.transpose(result, perm=[1,0,2])\n",
        "            \n",
        "        return result\n",
        "    \n",
        "    def zero_state(self, batch_size):\n",
        "\n",
        "        return (tf.zeros((batch_size, self.cell.units)), tf.zeros((batch_size, self.cell.units)))"
      ],
      "metadata": {
        "id": "WvXkYxi8YzOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM_Model(tf.keras.Model):\n",
        "\n",
        "   def __init__(self):\n",
        "       super(LSTM_Model, self).__init__()\n",
        "\n",
        "       self.read_in = Dense(64, activation='sigmoid')\n",
        "\n",
        "       self.dense_layer = Dense(32)\n",
        "\n",
        "       self.lstm_layer = LSTM_Layer(LSTM_Cell(2))\n",
        "\n",
        "       self.out = Dense(units=1, activation='sigmoid')\n",
        "   @tf.function\n",
        "   def call(self, x):\n",
        "       batch_size = tf.shape(x)[0]\n",
        "       #print(batch_size)\n",
        "       x = self.read_in(x)\n",
        "       zero_state = self.lstm_layer.zero_state(batch_size)\n",
        "       x = self.lstm_layer.call(x, zero_state)\n",
        "       x = self.out(x)\n",
        "\n",
        "       return x"
      ],
      "metadata": {
        "id": "0zonkD4VBK_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(model, input, target, loss_function, optimizer):\n",
        "  # loss_object and optimizer_object are instances of respective tensorflow classes\n",
        "  with tf.GradientTape() as tape:\n",
        "    prediction = model(input)\n",
        "    loss = loss_function(target, prediction[:,-1,:])\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "  return loss\n",
        "\n",
        "\n",
        "def test(model, test_data, loss_function):\n",
        "  # test over complete test data\n",
        "\n",
        "  test_accuracy_aggregator = []\n",
        "  test_loss_aggregator = []\n",
        "\n",
        "  for (input, target) in test_data:\n",
        "    prediction = model(input)\n",
        "    sample_test_loss = loss_function(target, prediction)\n",
        "    sample_test_accuracy =  np.round(target) == np.round(prediction[:,-1,:])\n",
        "    sample_test_accuracy = np.mean(sample_test_accuracy)\n",
        "    test_loss_aggregator.append(sample_test_loss.numpy())\n",
        "    test_accuracy_aggregator.append(np.mean(sample_test_accuracy))\n",
        "\n",
        "  test_loss = tf.reduce_mean(test_loss_aggregator)\n",
        "  test_accuracy = tf.reduce_mean(test_accuracy_aggregator)\n",
        "\n",
        "  return test_loss, test_accuracy"
      ],
      "metadata": {
        "id": "n0QY5sz0DJhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "train_dataset = train_ds \n",
        "test_dataset = test_ds \n",
        "\n",
        "### Hyperparameters\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Initialize the model.\n",
        "model = LSTM_Model()\n",
        "# Initialize the loss: binary cross entropy. Check out 'tf.keras.losses'.\n",
        "cross_entropy_loss = tf.keras.losses.BinaryCrossentropy()\n",
        "# Initialize the optimizer: Nadam with default parameters\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "# Initialize lists for later visualization.\n",
        "train_losses = []\n",
        "\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "#testing once before we begin\n",
        "test_loss, test_accuracy = test(model, test_dataset, cross_entropy_loss)\n",
        "test_losses.append(test_loss)\n",
        "test_accuracies.append(test_accuracy)\n",
        "\n",
        "#check how model performs on train data once before we begin\n",
        "train_loss, _ = test(model, train_dataset, cross_entropy_loss)\n",
        "train_losses.append(train_loss)\n",
        "\n",
        "# We train for num_epochs.\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'Epoch {str(epoch)} ::: Accuracy: {test_accuracies[epoch]} ::: Loss:{train_losses[epoch]}')\n",
        "    #training (and checking in with training)\n",
        "    epoch_loss_agg = []\n",
        "    for input,target in train_dataset:\n",
        "        train_loss = train_step(model, input, target, cross_entropy_loss, optimizer)\n",
        "        epoch_loss_agg.append(train_loss)\n",
        "    \n",
        "    #track training loss\n",
        "    train_losses.append(tf.reduce_mean(epoch_loss_agg))\n",
        "\n",
        "    #testing, so we can track accuracy and test loss\n",
        "    test_loss, test_accuracy = test(model, test_dataset, cross_entropy_loss)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracies.append(test_accuracy)\n",
        "print('Final accuracy:', test_accuracies[epoch])  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KY6iKN1EDNP0",
        "outputId": "2601cf5a-3f45-4bc0-e8cd-3eaf24a5516d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 ::: Accuracy: nan ::: Loss:0.6965903639793396\n",
            "Epoch 1 ::: Accuracy: nan ::: Loss:0.42759719491004944\n",
            "Epoch 2 ::: Accuracy: nan ::: Loss:0.2515907883644104\n",
            "Epoch 3 ::: Accuracy: nan ::: Loss:0.20742614567279816\n",
            "Epoch 4 ::: Accuracy: nan ::: Loss:0.1821606606245041\n",
            "Epoch 5 ::: Accuracy: nan ::: Loss:0.16359546780586243\n",
            "Epoch 6 ::: Accuracy: nan ::: Loss:0.1500256359577179\n",
            "Epoch 7 ::: Accuracy: nan ::: Loss:0.13891752064228058\n",
            "Epoch 8 ::: Accuracy: nan ::: Loss:0.12931491434574127\n",
            "Epoch 9 ::: Accuracy: nan ::: Loss:0.12094012647867203\n",
            "Final accuracy: tf.Tensor(nan, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8HzhlGrejIme"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}